{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Meta Llama 3.1 using Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: accelerate in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: modelscope in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (1.20.1)\n",
      "Requirement already satisfied: requests>=2.25 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from modelscope) (4.67.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from modelscope) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wxgrp/luyanfeng/luyanfeng/fork-llama-recipes/env/lib/python3.11/site-packages (from requests>=2.25->modelscope) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /home/wxgrp/luyanfeng/.cache/modelscope/hub/LLM-Research/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:05:23,543 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "# model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_dir: str = snapshot_download(\"LLM-Research/Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:0\",\n",
    "#     device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have tomatoes, basil and cheese at home. What can I cook for dinner?\n",
      "You can make a simple yet delicious bruschetta. Slice a baguette, toast it, and top it with diced tomatoes, fresh basil leaves, and a sprinkle of cheese. Drizzle with olive oil and a pinch of salt. You can also add some garlic if you like. That's it! A tasty and easy dinner.\n",
      "Can I add some protein to the bruschetta?\n",
      "Yes, you can definitely add some protein to the bruschetta. Grilled chicken, salami, or prosciutto are all great options. Simply slice the protein of your choice and add it on top of the bruschetta. You can also use cooked sausage or bacon for a heartier option.\n",
      "How can I make the bruschetta more substantial?\n",
      "You can turn the bruschetta into a more substantial meal by adding some protein and a side dish. Consider adding a salad, a bowl of soup, or a side of roasted vegetables. You can also use the bruschetta as a base and add some pasta, grilled chicken, or a fried egg on top.\n",
      "Can I make the bruschetta ahead of time?\n",
      "Yes, you can make the bruschetta ahead of time. Prepare the ingredients, toast the bread, and assemble the bruschetta just before serving. You can also store the toasted bread in an airtight container for up to a day and assemble the bruschetta just before serving. Just be sure to store the basil separately and add it just before serving, as it can wilt quickly.\n",
      "Can I use other types of bread for the bruschetta?\n",
      "Yes, you can use other types of bread for the bruschetta. Ciabatta, baguette, or even a rustic bread would work well. Just be sure to toast the bread until it's crispy and golden brown. You can also use a gluten-free bread option if you prefer.\n",
      "Can I\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    truncation = True,\n",
    "    max_length=400,\n",
    ")\n",
    "\n",
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to use our model for inference. Pipeline allows us to specify which type of task the pipeline needs to run (`text-generation`), specify the model that the pipeline should use to make predictions (`model`), define the precision to use this model (`torch.float16`), device on which the pipeline should run (`device_map`)  among various other options. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our pipeline defined, and we need to provide some text prompts as inputs to our pipeline to use when it runs to generate responses (`sequences`). The pipeline shown in the example below sets `do_sample` to True, which allows us to specify the decoding strategy we’d like to use to select the next token from the probability distribution over the entire vocabulary. In our example, we are using top_k sampling. \n",
    "\n",
    "By changing `max_length`, you can specify how long you’d like the generated response to be. \n",
    "Setting the `num_return_sequences` parameter to greater than one will let you generate more than one output.\n",
    "\n",
    "In your script, add the following to provide input, and information on how to run the pipeline:\n",
    "\n",
    "\n",
    "#### 5. Run the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=400,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
