{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Llama 3.1\n",
    "\n",
    "Prompt engineering is using natural language to produce a desired response from a large language model (LLM)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models\n",
    "\n",
    "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
    "\n",
    "Llama models come in varying parameter sizes. The smaller models are cheaper to deploy and run; the larger models are more capable.\n",
    "\n",
    "#### Llama 3.1\n",
    "1. `llama-3.1-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3.1-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3.1-405b` - base pretrained 405 billion parameter model\n",
    "1. `llama-3.1-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3.1-70b-instruct` - instruction fine-tuned 70 billion parameter model\n",
    "1. `llama-3.1-405b-instruct` - instruction fine-tuned 405 billion parameter model (flagship)\n",
    "\n",
    "\n",
    "#### Llama 3\n",
    "1. `llama-3-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3-70b-instruct` - instruction fine-tuned 70 billion parameter model (flagship)\n",
    "\n",
    "#### Llama 2\n",
    "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
    "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
    "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
    "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
    "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
    "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
    "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
    "1. `codellama-70b` - code fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-instruct` - code & instruct fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-python` - Python fine-tuned 70 billion parameter model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sky appears blue to us because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the Earth\\'s atmosphere. Here\\'s a simplified explanation:\\n\\n1. **Sunlight enters the atmosphere**: When the sun shines, it emits all types of electromagnetic radiation, including visible light. This light is made up of different colors, which we can see as white light.\\n2. **Scattering occurs**: As sunlight travels through the Earth\\'s atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of visible light. When sunlight hits these molecules, it scatters in all directions, but not uniformly.\\n3. **Short wavelengths scattered more**: The shorter wavelengths of light, which we perceive as blue and violet, are scattered more than the longer wavelengths (red and orange) because they have higher energies and can penetrate more easily through the smaller gaps between the gas molecules.\\n4. **Blue light dominates our view**: As a result of this scattering, the blue light is dispersed in all directions and reaches our eyes from every point in the sky. Our atmosphere acts as a kind of \"blue filter,\" making the sky appear blue to us.\\n\\nThis phenomenon is known as Rayleigh scattering, named after Lord Rayleigh, who first described it in the late 19th century. It\\'s worth noting that the exact shade of blue we see can vary depending on factors like:\\n\\n* **Time of day**: The color of the sky can change throughout the day due to the angle of the sun and the amount of scattered light.\\n* **Atmospheric conditions**: Dust, pollution, and water vapor in the atmosphere can affect the scattering of light and alter the apparent color of the sky.\\n* **Location on Earth**: The blue color may appear more intense near large bodies of water or in areas with less atmospheric pollution.\\n\\nNow, go outside and appreciate that beautiful blue sky!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    options={'temperature': 0.8, 'top_p': 1.0}\n",
    ")\n",
    "\n",
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the atmosphere. Here's why:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When the sun shines, it sends out all its colors, including red, orange, yellow, green, blue, and violet.\n",
      "2. **Scattering occurs**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light, so they scatter the light in all directions.\n",
      "3. **Short wavelengths scattered more**: The blue light, with its shorter wavelength (around 450-495 nanometers), is scattered more efficiently by these gas molecules than the longer wavelengths like red and orange light. This is known as Rayleigh scattering, named after the British physicist who first described it.\n",
      "4. **Blue light reaches our eyes**: Because of this increased scattering, much of the blue light is dispersed in all directions, reaching our eyes from every part of the sky. In contrast, the longer wavelengths (like red and orange) continue to travel in a more direct path, which means they reach our eyes mainly from the direction of the sun.\n",
      "5. **Our brain interprets the scattered light as blue**: When we look up at the sky, our brain combines all these scattered blue photons with the fainter, longer-wavelength light that's not so intensely scattered. This results in the blue color we see.\n",
      "\n",
      "Other factors can influence the apparent blue color of the sky:\n",
      "\n",
      "* **Dust and water particles**: Tiny particles in the air, like dust or water droplets (as in clouds), can scatter sunlight and change its apparent color.\n",
      "* **Time of day and year**: The angle at which sunlight enters the atmosphere changes throughout the day and between seasons. This affects how much scattering occurs, resulting in variations in sky color.\n",
      "* **Atmospheric conditions**: Pollution, aerosols, or other atmospheric conditions can alter the scattering process, making the sky appear more hazy or take on different hues.\n",
      "\n",
      "So, to summarize: the sky appears blue due to the way light scatters off tiny gas molecules in the atmosphere, with shorter wavelengths (like blue) being scattered more efficiently and reaching our eyes from all directions."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "DEFAULT_MODEL = \"llama3.1\"\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options={'temperature': temperature, 'top_p': top_p}\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    print(response, end='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion APIs\n",
    "\n",
    "Let's try Llama 3.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "The typical color of the sky is: \n",
      "==============\n",
      "The typical color of the sky varies depending on the time of day and atmospheric conditions. However, under normal circumstances:\n",
      "\n",
      "* During the daytime (when the sun is overhead), the typical color of the sky is **blue**. This is due to a phenomenon called Rayleigh scattering, where shorter wavelengths of light (like blue and violet) are scattered more than longer wavelengths (like red and orange) by the tiny molecules of gases in the atmosphere.\n",
      "* During sunrise and sunset, when the sun's rays pass through more of the Earth's atmosphere, the typical color of the sky is **orange** or **pink**, due to the scattering of light by atmospheric particles and water vapor.\n",
      "* At night, the sky appears **dark gray** or **black**, with stars and moon visible against a dark background.\n",
      "\n",
      "Of course, these colors can change depending on weather conditions like clouds, pollution, and dust. But under normal circumstances, blue is the typical color of the daytime sky!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"The typical color of the sky is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "which model version are you?\n",
      "==============\n",
      "I'm an AI, and I don't have a specific \"model version\" in the classical sense. However, I can tell you that I'm based on a transformer architecture, which is a type of neural network designed for natural language processing tasks.\n",
      "\n",
      "My training data is based on a massive corpus of text from various sources, including but not limited to:\n",
      "\n",
      "* Web pages\n",
      "* Books and academic papers\n",
      "* Product reviews and forums\n",
      "* Social media platforms\n",
      "\n",
      "I was trained on a large-scale dataset that includes over 1 trillion parameters, which allows me to generate human-like responses to a wide range of questions and topics.\n",
      "\n",
      "As for my specific \"model version,\" I'm based on the following:\n",
      "\n",
      "* **LLaMA (Large Language Model Application)**: My underlying model architecture is inspired by LLaMA, an open-source transformer-based language model developed by Meta AI.\n",
      "* **Transformers 3.1**: My training data and model weights are based on the Transformers 3.1 framework, which provides a modular and efficient way to implement transformer models.\n",
      "\n",
      "Keep in mind that I'm constantly learning and improving from user interactions like this one!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"which model version are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion APIs\n",
    "Chat completion models provide additional structure to interacting with an LLM. An array of structured message objects is sent to the LLM instead of a single piece of text. This message list provides the LLM with some \"context\" or \"history\" from which to continue.\n",
    "\n",
    "Typically, each message contains `role` and `content`:\n",
    "* Messages with the `system` role are used to provide core instruction to the LLM by developers.\n",
    "* Messages with the `user` role are typically human-provided messages.\n",
    "* Messages with the `assistant` role are typically generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You told me earlier that your favorite color is blue!\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"My favorite color is blue.\"),\n",
    "    assistant(\"That's great to hear!\"),\n",
    "    user(\"What is my favorite color?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Hyperparameters\n",
    "\n",
    "#### `temperature` & `top_p`\n",
    "\n",
    "These APIs also take parameters which influence the creativity and determinism of your output.\n",
    "\n",
    "At each step, LLMs generate a list of most likely tokens and their respective probabilities. The least likely tokens are \"cut\" from the list (based on `top_p`), and then a token is randomly selected from the remaining candidates (`temperature`).\n",
    "\n",
    "In other words: `top_p` controls the breadth of vocabulary in a generation and `temperature` controls the randomness within that vocabulary. A temperature of ~0 produces *almost* deterministic results.\n",
    "\n",
    "[Read more about temperature setting here](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy, gentle soul\n",
      "Llama's soft eyes gaze at me\n",
      "Peaceful Andean heart\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy, gentle soul\n",
      "Llama's soft eyes gaze at me\n",
      "Peaceful Andean heart\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Softly fuzzy face\n",
      "Trotting through Andean hills\n",
      "Nature's gentle soul\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy camel backs\n",
      "Llamas softly munch the grass\n",
      "Peaceful Andean hills\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tuned_completion(temperature: float, top_p: float):\n",
    "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
    "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
    "\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "# These two generations are highly likely to be the same\n",
    "\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "# These two generations are highly likely to be different"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Instructions\n",
    "\n",
    "Detailed, explicit instructions produce better results than open-ended prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Describe quantum physics in one short sentence of no more than 12 words\n",
      "==============\n",
      "Quantum physics studies the behavior of matter and energy at atomic level.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
    "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about giving explicit instructions as using rules and restrictions to how Llama 3 responds to your prompt.\n",
    "\n",
    "- Stylization\n",
    "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
    "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
    "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
    "- Formatting\n",
    "    - `Use bullet points.`\n",
    "    - `Return as a JSON object.`\n",
    "    - `Use less technical terms and help me apply it in my work in communications.`\n",
    "- Restrictions\n",
    "    - `Only use academic papers.`\n",
    "    - `Never give sources older than 2020.`\n",
    "    - `If you don't know the answer, say that you don't know.`\n",
    "\n",
    "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the latest advances in large language models to me.\n",
      "==============\n",
      "The field of natural language processing (NLP) has seen tremendous progress in recent years, particularly with the development and application of large language models (LLMs). Here are some of the latest advances:\n",
      "\n",
      "**Transformers and Attention Mechanisms**\n",
      "\n",
      "In 2017, Vaswani et al. introduced the Transformer architecture, which revolutionized sequence-to-sequence tasks like machine translation, text summarization, and question answering. The Transformer's attention mechanism allows the model to weigh the importance of different input elements when generating output. This innovation has become a cornerstone for many LLMs.\n",
      "\n",
      "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
      "\n",
      "In 2018, Devlin et al. introduced BERT, a pre-trained language model that achieved state-of-the-art results on various NLP tasks. BERT's success lies in its ability to learn bidirectional contextualized representations of input tokens using the Transformer architecture. This allows the model to capture complex relationships between words and phrases.\n",
      "\n",
      "**Improvements and Variants**\n",
      "\n",
      "Since BERT, numerous variants have been proposed to improve upon or extend its capabilities:\n",
      "\n",
      "1. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: A modified version of BERT that uses a different optimization technique and achieves better results on some tasks.\n",
      "2. **DistilBERT**: A smaller, more efficient version of BERT that still maintains high accuracy.\n",
      "3. **XLNet**: An LLM that uses an autoregressive model to generate output and has achieved state-of-the-art results on various NLP tasks.\n",
      "4. **ALBERT (A Lite BERT)**: Another variant that uses a different attention mechanism and achieves better results while being more computationally efficient.\n",
      "\n",
      "**Scaling Up**\n",
      "\n",
      "To further improve performance, researchers have explored scaling up LLMs by:\n",
      "\n",
      "1. **Increasing Model Size**: Larger models with more parameters can learn more complex relationships between input tokens.\n",
      "2. **Multi-Task Learning**: Training the model on multiple tasks simultaneously to improve generalization and robustness.\n",
      "3. **Knowledge Distillation**: Using a smaller student model to mimic the behavior of a larger teacher model, allowing for knowledge transfer and improved performance.\n",
      "\n",
      "**Recent Advances**\n",
      "\n",
      "Some recent advances in LLMs include:\n",
      "\n",
      "1. **Long-Range Dependencies**: Models can now capture long-range dependencies between input tokens, improving performance on tasks like text classification and sentiment analysis.\n",
      "2. **Common Sense Reasoning**: Models have been shown to demonstrate common sense reasoning capabilities, such as understanding everyday situations and making logical conclusions.\n",
      "3. **Conversational AI**: LLMs are being used in conversational interfaces, enabling more natural and engaging interactions between humans and machines.\n",
      "\n",
      "**Future Directions**\n",
      "\n",
      "The field of LLMs is rapidly evolving, with several promising areas of research:\n",
      "\n",
      "1. **Explainability**: Developing techniques to understand how LLMs arrive at their decisions, improving transparency and trustworthiness.\n",
      "2. **Adversarial Robustness**: Improving models' resistance to manipulation and attacks from adversarial sources.\n",
      "3. **Multimodal Learning**: Combining language understanding with other modalities, such as vision or audio, to create more comprehensive AI systems.\n",
      "\n",
      "These are just a few of the latest advances in large language models. The field is rapidly evolving, and we can expect even more exciting developments in the near future!\n",
      "\n",
      "==============\n",
      "Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\n",
      "==============\n",
      "The field of large language models (LLMs) has seen significant advancements in recent years, with a focus on improving model performance, efficiency, and interpretability. Here are some of the latest advances:\n",
      "\n",
      "1. **Scaling laws for transformer-based LLMs**: Researchers have discovered that the scaling law for transformer-based LLMs follows a power-law relationship between model size and performance (Kaplan et al., 2020). This means that as the model size increases, its performance improves at an exponential rate.\n",
      "\n",
      "Source: Kaplan, F., et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2009.01431 (2020)\n",
      "\n",
      "2. **Improved efficiency through sparse attention**: Sparse attention mechanisms have been proposed to reduce the computational cost of LLMs while maintaining performance (Child et al., 2021). This approach involves selecting a subset of relevant input tokens for attention, rather than computing attention weights for all tokens.\n",
      "\n",
      "Source: Child, W., et al. \"Sparse attention is all you need.\" arXiv preprint arXiv:2102.09532 (2021)\n",
      "\n",
      "3. **Advancements in multimodal LLMs**: Researchers have explored the integration of vision and language models to create multimodal LLMs that can process both text and image inputs (Lu et al., 2020). These models have been shown to improve performance on tasks such as visual question answering.\n",
      "\n",
      "Source: Lu, J., et al. \"ViLT: Vision-and-language Transformers for Common Sense Reasoning.\" arXiv preprint arXiv:2007.08409 (2020)\n",
      "\n",
      "4. **Efficient LLMs through knowledge distillation**: Knowledge distillation is a technique that involves transferring knowledge from a large teacher model to a smaller student model, reducing the computational cost while maintaining performance (Jiao et al., 2021). This approach has been shown to be effective in various tasks, including language translation and text classification.\n",
      "\n",
      "Source: Jiao, X., et al. \"Squeeze-and-Excitation Attention Networks for Efficient LLMs.\" arXiv preprint arXiv:2104.09664 (2021)\n",
      "\n",
      "5. **Advances in interpretability**: Researchers have proposed methods to improve the interpretability of LLMs, such as attention visualization and feature importance analysis (Bai et al., 2020). These approaches enable users to understand how the model arrives at its predictions.\n",
      "\n",
      "Source: Bai, X., et al. \"Attention-Weighted Feature Importance for Large Language Models.\" arXiv preprint arXiv:2007.09431 (2020)\n",
      "\n",
      "6. **Advancements in few-shot learning**: Few-shot learning involves training models on a small dataset and achieving high performance on unseen data. Researchers have proposed methods to improve the efficiency of few-shot learning, such as using meta-learning algorithms and attention mechanisms (Chen et al., 2021).\n",
      "\n",
      "Source: Chen, Y., et al. \"Efficient Few-Shot Learning through Meta-Learning and Attention.\" arXiv preprint arXiv:2102.11493 (2021)\n",
      "\n",
      "These advances demonstrate the rapid progress being made in the field of large language models, with a focus on improving efficiency, interpretability, and performance.\n",
      "\n",
      "References:\n",
      "\n",
      "* Kaplan, F., et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2009.01431 (2020)\n",
      "* Child, W., et al. \"Sparse attention is all you need.\" arXiv preprint arXiv:2102.09532 (2021)\n",
      "* Lu, J., et al. \"ViLT: Vision-and-language Transformers for Common Sense Reasoning.\" arXiv preprint arXiv:2007.08409 (2020)\n",
      "* Jiao, X., et al. \"Squeeze-and-Excitation Attention Networks for Efficient LLMs.\" arXiv preprint arXiv:2104.09664 (2021)\n",
      "* Bai, X., et al. \"Attention-Weighted Feature Importance for Large Language Models.\" arXiv preprint arXiv:2007.09431 (2020)\n",
      "* Chen, Y., et al. \"Efficient Few-Shot Learning through Meta-Learning and Attention.\" arXiv preprint arXiv:2102.11493 (2021)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the latest advances in large language models to me.\")\n",
    "# More likely to cite sources from 2017\n",
    "\n",
    "complete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n",
    "# Gives more specific advances and only cites sources from 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Prompting using Zero- and Few-Shot Learning\n",
    "\n",
    "A shot is an example or demonstration of what type of prompt and response you expect from a large language model. This term originates from training computer vision models on photographs, where one shot was one example or instance that the model used to classify an image ([Fei-Fei et al. (2006)](http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf)).\n",
    "\n",
    "#### Zero-Shot Prompting\n",
    "\n",
    "Large language models like Llama 3 are unique because they are capable of following instructions and producing responses without having previously seen an example of a task. Prompting without examples is called \"zero-shot prompting\".\n",
    "\n",
    "Let's try using Llama 3 as a sentiment detector. You may notice that output format varies - we can improve this with better prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Text: This was the best movie I've ever seen! \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "**Extremely Positive**\n",
      "\n",
      "The text expresses an overwhelmingly positive and enthusiastic opinion about a movie, using superlatives like \"best\" to emphasize its exceptional quality.\n",
      "\n",
      "==============\n",
      "Text: The director was trying too hard. \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "Disapproval or criticism.\n",
      "\n",
      "The word \"trying\" implies an effort to achieve something, but in this context, it's used in a negative way to suggest that the director is over-exerting themselves and not doing things naturally. The phrase \"too hard\" further reinforces this sentiment, implying that the director's efforts are excessive and perhaps even awkward or forced.\n",
      "\n",
      "Overall, the tone of the text suggests that the writer has been disappointed or unimpressed with the director's work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
    "# Returns positive sentiment\n",
    "\n",
    "complete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
    "# Returns negative sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Few-Shot Prompting\n",
    "\n",
    "Adding specific examples of your desired output generally results in more accurate, consistent output. This technique is called \"few-shot prompting\".\n",
    "\n",
    "In this example, the generated response follows our desired format that offers a more nuanced sentiment classifer that gives a positive, neutral, and negative response confidence percentage.\n",
    "\n",
    "See also: [Zhao et al. (2021)](https://arxiv.org/abs/2102.09690), [Liu et al. (2021)](https://arxiv.org/abs/2101.06804), [Su et al. (2022)](https://arxiv.org/abs/2209.01975), [Rubin et al. (2022)](https://arxiv.org/abs/2112.08633).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: I thought it was okay\n",
      "10% positive 80% neutral 10% negative\n",
      "INPUT: I loved it!\n",
      "95% positive 5% neutral 0% negative\n",
      "INPUT: Terrible service 0/10\n",
      "5% positive 0% neutral 95% negative\n"
     ]
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    response = chat_completion(messages=[\n",
    "        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n",
    "        user(\"I liked it\"),\n",
    "        assistant(\"70% positive 30% neutral 0% negative\"),\n",
    "        user(\"It could be better\"),\n",
    "        assistant(\"0% positive 50% neutral 50% negative\"),\n",
    "        user(\"It's fine\"),\n",
    "        assistant(\"25% positive 50% neutral 25% negative\"),\n",
    "        user(text),\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "def print_sentiment(text):\n",
    "    print(f'INPUT: {text}')\n",
    "    print(sentiment(text))\n",
    "\n",
    "print_sentiment(\"I thought it was okay\")\n",
    "# More likely to return a balanced mix of positive, neutral, and negative\n",
    "print_sentiment(\"I loved it!\")\n",
    "# More likely to return 100% positive\n",
    "print_sentiment(\"Terrible service 0/10\")\n",
    "# More likely to return 100% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Llama will often give more consistent responses when given a role ([Kong et al. (2023)](https://browse.arxiv.org/pdf/2308.07702.pdf)). Roles give context to the LLM on what type of answers are desired.\n",
    "\n",
    "Let's use Llama 3 to create a more focused, technical response for a question around the pros and cons of using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "PyTorch is a popular open-source machine learning library developed by Facebook's AI Research Lab (FAIR). Here are some pros and cons of using PyTorch:\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Dynamic computation graph**: Unlike other deep learning frameworks like TensorFlow, PyTorch uses a dynamic computation graph, which means that the graph is built on-the-fly when you run your code. This makes it easier to debug and understand what's happening in your model.\n",
      "2. **Ease of use**: PyTorch has a simple and intuitive API that makes it easy to get started with deep learning. It also supports Python-like syntax, making it feel more like traditional Python programming than other ML frameworks.\n",
      "3. **Flexibility**: PyTorch allows you to mix-and-match different types of modules (e.g., convolutional, recurrent, attention) in a single model, which is useful for experimenting with new architectures.\n",
      "4. **Rapid prototyping**: PyTorch's dynamic computation graph and ease of use make it ideal for rapid prototyping and experimentation.\n",
      "5. **Large community**: PyTorch has a growing and active community, with many pre-trained models available on popular repositories like Hugging Face's Transformers.\n",
      "6. **Native support for GPUs**: PyTorch supports native GPU acceleration through the CUDA framework (for NVIDIA GPUs) and ROCm (for AMD GPUs), making it suitable for large-scale deep learning tasks.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Limited production-readiness**: While PyTorch is great for research and prototyping, it's not yet as mature or widely adopted in production environments as TensorFlow.\n",
      "2. **Lack of strong typing**: Unlike some other ML frameworks (e.g., TensorFlow 2.x), PyTorch does not enforce strong typing by default. This can lead to runtime errors if you're not careful with your code.\n",
      "3. **Memory management**: PyTorch requires manual memory management, which can be error-prone and lead to issues like memory leaks or crashes.\n",
      "4. **Limited support for distributed training**: While PyTorch supports distributed training through the `torch.distributed` module, it's still not as robust or widely adopted as TensorFlow's distributed training capabilities.\n",
      "5. **Not as extensive a set of pre-built functions**: Compared to other ML frameworks (e.g., scikit-learn), PyTorch has fewer pre-built functions and tools for tasks like feature engineering, data preprocessing, or model selection.\n",
      "6. **Less comprehensive documentation**: While PyTorch's documentation is generally good, it can be harder to find detailed information on specific topics compared to other ML frameworks.\n",
      "\n",
      "**In conclusion**, PyTorch is a great choice for researchers, prototypers, and developers who want to quickly experiment with new ideas or architectures. However, if you're working in a production environment or need more robust features like strong typing, memory management, or distributed training, you may want to consider other ML frameworks (e.g., TensorFlow).\n",
      "\n",
      "==============\n",
      "Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "As a machine learning expert, I've had the pleasure of working with various frameworks, including PyTorch. Here's a detailed breakdown of its pros and cons:\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Dynamic Computation Graph**: PyTorch's dynamic computation graph allows for more flexibility in model design and modification during training. This feature is particularly useful when experimenting with different architectures or techniques.\n",
      "2. **Automatic Differentiation**: PyTorch's automatic differentiation (AD) system enables efficient computation of gradients, which is critical for optimizing deep neural networks. The AD engine can handle complex computations, including those involving custom operators and functions.\n",
      "3. **GPU Acceleration**: PyTorch provides seamless integration with NVIDIA GPUs, making it an ideal choice for training large-scale models. Its GPU acceleration capabilities are on par with those of TensorFlow and Keras.\n",
      "4. **Pythonic API**: The PyTorch API is designed to be highly Pythonic, which means it's easy to read and write code that's similar to native Python. This makes it a joy to work with, especially for developers already familiar with the language.\n",
      "5. **Community Support**: PyTorch has an active community of users and contributors, which translates to a wealth of documentation, tutorials, and pre-built models available online.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Resource Intensiveness**: Training large-scale models on GPUs can be computationally expensive, even with PyTorch's efficient memory management. This may lead to high energy consumption, heat dissipation issues, or even hardware failures.\n",
      "2. **Limited Support for Distributed Training**: While PyTorch does support distributed training, its implementation is not as polished as that of other frameworks like TensorFlow or Horovod. This might limit its adoption in large-scale production environments.\n",
      "3. **Debugging Challenges**: The dynamic computation graph and automatic differentiation system can make debugging more difficult, especially when dealing with complex models or custom operators.\n",
      "4. **Limited Support for Pre-trained Models**: Compared to other popular frameworks, PyTorch has limited support for pre-trained models, which might be a drawback for users looking to leverage existing architectures or fine-tune them on specific tasks.\n",
      "5. **Less Mature Ecosystem**: While the PyTorch ecosystem is growing rapidly, it still lags behind that of TensorFlow in terms of maturity and breadth of available tools, libraries, and integrations.\n",
      "\n",
      "**When to Choose PyTorch:**\n",
      "\n",
      "1. **Research and Development**: PyTorch's flexibility and ease of use make it an ideal choice for researchers and developers looking to experiment with novel architectures or techniques.\n",
      "2. **Small- to Medium-Scale Projects**: For projects involving smaller datasets or simpler models, PyTorch's lightweight and efficient nature can be a significant advantage.\n",
      "3. **Custom Model Development**: When developing custom models that require complex computations or custom operators, PyTorch's dynamic computation graph and automatic differentiation system can provide significant benefits.\n",
      "\n",
      "**When to Avoid PyTorch:**\n",
      "\n",
      "1. **Large-Scale Production Environments**: For large-scale production environments with strict scalability and reliability requirements, TensorFlow or other more mature frameworks might be a better choice.\n",
      "2. **High-Performance Computing (HPC) Applications**: When working on HPC applications that require extreme computational resources, frameworks like TensorFlow or Keras might be more suitable due to their more mature support for distributed training and GPU acceleration.\n",
      "\n",
      "In conclusion, PyTorch is an excellent choice for research and development, small- to medium-scale projects, or custom model development. However, its limitations in resource intensiveness, limited support for distributed training, and debugging challenges might make it less ideal for large-scale production environments or HPC applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the pros and cons of using PyTorch.\")\n",
    "# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n",
    "\n",
    "complete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought\n",
    "\n",
    "Simply adding a phrase encouraging step-by-step thinking \"significantly improves the ability of large language models to perform complex reasoning\" ([Wei et al. (2022)](https://arxiv.org/abs/2201.11903)). This technique is called \"CoT\" or \"Chain-of-Thought\" prompting.\n",
    "\n",
    "Llama 3.1 now reasons step-by-step naturally without the addition of the phrase. This section remains for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Who lived longer, Mozart or Elvis?\n",
      "==============\n",
      "A fun comparison!\n",
      "\n",
      "Wolfgang Amadeus Mozart (1756-1791) died at the age of 35.\n",
      "\n",
      "Elvis Presley (1935-1977) died at the age of 42.\n",
      "\n",
      "So, Elvis outlived Mozart by about 7 years.\n",
      "\n",
      "==============\n",
      "Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\n",
      "==============\n",
      "A fun comparison!\n",
      "\n",
      "Let's break it down step by step:\n",
      "\n",
      "1. **Birthdate**: Wolfgang Amadeus Mozart was born on January 27, 1756.\n",
      "2. **Death date**: Mozart passed away on December 5, 1791, at the age of 35.\n",
      "\n",
      "Next, let's consider Elvis Presley:\n",
      "\n",
      "1. **Birthdate**: Elvis Aaron Presley was born on January 8, 1935.\n",
      "2. **Death date**: Elvis Presley died on August 16, 1977, at the age of 42.\n",
      "\n",
      "Now, let's calculate the difference in their lifespans:\n",
      "\n",
      "Mozart (1756-1791) = 35 years\n",
      "Elvis (1935-1977) = 42 years\n",
      "\n",
      "Therefore, **Elvis Presley lived longer than Wolfgang Amadeus Mozart**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who lived longer, Mozart or Elvis?\"\n",
    "\n",
    "complete_and_print(prompt)\n",
    "# Llama 2 would often give the incorrect answer of \"Mozart\"\n",
    "\n",
    "complete_and_print(f\"{prompt} Let's think through this carefully, step by step.\")\n",
    "# Gives the correct answer \"Elvis\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "LLMs are probablistic, so even with Chain-of-Thought, a single generation might produce incorrect results. Self-Consistency ([Wang et al. (2022)](https://arxiv.org/abs/2203.11171)) introduces enhanced accuracy by selecting the most frequent answer from multiple generations (at the cost of higher compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: ['50', '50', '50', '50', '440']\n",
      "Final answer: 50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by backticks (example: `123`)\",\n",
    "    )\n",
    "    match = re.search(r'`(\\d+)`', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(f\"Answers: {answers}\\nFinal answer: {mode(answers)}\")\n",
    "\n",
    "# Sample runs of Llama-3-70B (all correct):\n",
    "# ['60', '50', '50', '50', '50'] -> 50\n",
    "# ['50', '50', '50', '60', '50'] -> 50\n",
    "# ['50', '50', '60', '50', '50'] -> 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "You'll probably want to use factual knowledge in your application. You can extract common facts from today's large models out-of-the-box (i.e. using just the model weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"What is the capital of the California?\")\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more specific facts, or private information, cannot be reliably retrieved. The model will either declare it does not know or hallucinate an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n",
    "# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n",
    "\n",
    "complete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n",
    "# \"I'm not able to access your personal information [..] I can provide some general guidance\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation, or RAG, describes the practice of including information in the prompt you've retrived from an external database ([Lewis et al. (2020)](https://arxiv.org/abs/2005.11401v4)). It's an effective way to incorporate facts into your LLM application and is more affordable than fine-tuning which may be costly and negatively impact the foundational model's capabilities.\n",
    "\n",
    "This could be as simple as a lookup table or as sophisticated as a [vector database]([FAISS](https://github.com/facebookresearch/faiss)) containing all of your company's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-Aided Language Models\n",
    "\n",
    "LLMs, by nature, aren't great at performing calculations. Let's try:\n",
    "\n",
    "$$\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "$$\n",
    "\n",
    "(The correct answer is 91383.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "# Gives incorrect answers like 92448, 92648, 95463"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gao et al. (2022)](https://arxiv.org/abs/2211.10435) introduced the concept of \"Program-aided Language Models\" (PAL). While LLMs are bad at arithmetic, they're great for code generation. PAL leverages this fact by instructing the LLM to write code to solve calculation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was generated by Llama 3 70B:\n",
    "\n",
    "result = ((-5 + 93 * 4 - 0) * (4**4 - 7 + 0 * 5))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle with Llama 2 is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\"), even if explicit instructions are given to Llama 2 to be concise and no preamble. Llama 3.x can better follow instructions.\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    ")\n",
    "# \"{'zip_code': 94025}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
    "- [LearnPrompting.org](https://learnprompting.org/)\n",
    "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author & Contact\n",
    "\n",
    "Edited by [Dalton Flanagan](https://www.linkedin.com/in/daltonflanagan/) (dalton@meta.com) with contributions from Mohsen Agsen, Bryce Bortree, Ricardo Juan Palma Duran, Kaolin Fire, Thomas Scialom."
   ]
  }
 ],
 "metadata": {
  "captumWidgetMessage": [],
  "dataExplorerConfig": [],
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
  "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
  "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
  "operator_data": [],
  "outputWidgetContext": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
