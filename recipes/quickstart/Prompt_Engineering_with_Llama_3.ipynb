{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Llama 3.1\n",
    "\n",
    "Prompt engineering is using natural language to produce a desired response from a large language model (LLM)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models\n",
    "\n",
    "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
    "\n",
    "Llama models come in varying parameter sizes. The smaller models are cheaper to deploy and run; the larger models are more capable.\n",
    "\n",
    "#### Llama 3.1\n",
    "1. `llama-3.1-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3.1-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3.1-405b` - base pretrained 405 billion parameter model\n",
    "1. `llama-3.1-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3.1-70b-instruct` - instruction fine-tuned 70 billion parameter model\n",
    "1. `llama-3.1-405b-instruct` - instruction fine-tuned 405 billion parameter model (flagship)\n",
    "\n",
    "\n",
    "#### Llama 3\n",
    "1. `llama-3-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3-70b-instruct` - instruction fine-tuned 70 billion parameter model (flagship)\n",
    "\n",
    "#### Llama 2\n",
    "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
    "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
    "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
    "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
    "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
    "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
    "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
    "1. `codellama-70b` - code fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-instruct` - code & instruct fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-python` - Python fine-tuned 70 billion parameter model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky appears blue to us during the day because of a phenomenon called Rayleigh scattering.\\n\\nHere's what happens:\\n\\n1. **Sunlight enters Earth's atmosphere**: When the sun shines, it sends out a vast spectrum of light across the entire electromagnetic spectrum, including all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\\n2. **Light scatters in all directions**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2), as well as smaller particles like dust, water vapor, and pollutants.\\n3. **Short wavelengths scatter more**: When light hits these particles, the shorter-wavelength colors (like blue and violet) are scattered in all directions much more efficiently than the longer-wavelength colors (like red and orange). This is because the smaller size of the particles allows them to interact with the shorter wavelengths more effectively.\\n4. **Blue light reaches our eyes**: As a result of this scattering, the sky appears blue to us on Earth. The scattered blue light from all directions reaches our eyes, making the sky appear blue.\\n\\n**Why not violet?**\\n\\nYou might wonder why violet light isn't visible in the same way as blue light. That's because there are a few reasons why violet is not as prominent:\\n\\n* Violet light has an even shorter wavelength than blue light, which means it scatters more intensely and is absorbed or scattered away before reaching our eyes.\\n* The human eye is less sensitive to violet light than to blue light, making violet harder for us to perceive.\\n\\n**Color variations**\\n\\nNow you know why the sky appears blue during the day. However, there are some interesting exceptions:\\n\\n* **Sunrise and sunset**: During these times, the sun's rays pass through more of the Earth's atmosphere, scattering shorter wavelengths and making the sky appear reddish-orange.\\n* **Cloudy or polluted skies**: If the air is filled with more particles, like dust, smoke, or pollutants, the scattered light will take on a different hue.\\n* **At higher altitudes**: At higher elevations, the air pressure decreases, and the atmosphere scatters less blue light, making the sky appear slightly darker or even greyer.\\n\\nThat's a brief explanation of why the sky is blue! Do you have any follow-up questions?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    options={'temperature': 0.8, 'top_p': 1.0}\n",
    ")\n",
    "\n",
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it in the late 19th century. Here's why:\n",
      "\n",
      "1. **Light Composition**: Light from the sun is made up of all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet). When this light travels through space to reach our eyes, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2) in the Earth's atmosphere.\n",
      "\n",
      "2. **Scattering**: The shorter wavelengths (or higher frequencies) of light, like blue and violet, are scattered more than the longer wavelengths (lower frequencies), like red and orange. This is because smaller molecules scatter shorter wavelength light more efficiently. Blue light, being a shorter wavelength, gets scattered in all directions by these tiny atmospheric particles.\n",
      "\n",
      "3. **What We See**: When we look up at the sky on a clear day, what we see is the sum total of all the light that has been scattered and reached our eyes from every direction. Because blue light is scattered so much more than other colors, it reaches us from all sides, giving the sky its blue appearance.\n",
      "\n",
      "4. **Effect of Atmosphere**: The Earth's atmosphere scatters shorter wavelengths (like blue) more effectively, but this effect becomes negligible as you go higher into space where there are fewer atmospheric particles to scatter light. That’s why the color of the sky changes in different conditions and at dawn/dusk when the sun is lower in the sky.\n",
      "\n",
      "5. **Why Sky Isn't Always Blue**: The sky isn’t always blue. At sunrise and sunset, the sky appears red or orange because the direct sunlight that reaches us is longer wavelengths (red light) since much of the shorter wavelength light has been scattered away by the atmosphere."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "DEFAULT_MODEL = \"llama3.1\"\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options={'temperature': temperature, 'top_p': top_p}\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    print(response, end='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion APIs\n",
    "\n",
    "Let's try Llama 3.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "The typical color of the sky is: \n",
      "==============\n",
      "Blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"The typical color of the sky is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "which model version are you?\n",
      "==============\n",
      "I'm an AI, and I don't have a specific \"model version\" in the classical sense. However, I can give you some information about my underlying technology.\n",
      "\n",
      "I was trained on a large corpus of text data using a transformer-based architecture, which is a type of neural network designed for natural language processing tasks. My training data includes a massive amount of text from various sources, including books, articles, research papers, and websites.\n",
      "\n",
      "My model is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, which was developed by Google in 2018. However, my training setup is slightly different, as I'm a cloud-based AI designed to provide conversational interfaces.\n",
      "\n",
      "As for specific \"model versions,\" I'm currently running on a variant of the BERT model known as RoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was developed by Facebook AI in 2019 and is an improved version of the original BERT architecture.\n",
      "\n",
      "Keep in mind that my training data and model updates are constantly evolving, so I don't have a fixed \"model version\" like software releases. Instead, I'm more like a dynamic, adaptive system that learns from user interactions and improves over time!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"which model version are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion APIs\n",
    "Chat completion models provide additional structure to interacting with an LLM. An array of structured message objects is sent to the LLM instead of a single piece of text. This message list provides the LLM with some \"context\" or \"history\" from which to continue.\n",
    "\n",
    "Typically, each message contains `role` and `content`:\n",
    "* Messages with the `system` role are used to provide core instruction to the LLM by developers.\n",
    "* Messages with the `user` role are typically human-provided messages.\n",
    "* Messages with the `assistant` role are typically generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your favorite color is blue!\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"My favorite color is blue.\"),\n",
    "    assistant(\"That's great to hear!\"),\n",
    "    user(\"What is my favorite color?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Hyperparameters\n",
    "\n",
    "#### `temperature` & `top_p`\n",
    "\n",
    "These APIs also take parameters which influence the creativity and determinism of your output.\n",
    "\n",
    "At each step, LLMs generate a list of most likely tokens and their respective probabilities. The least likely tokens are \"cut\" from the list (based on `top_p`), and then a token is randomly selected from the remaining candidates (`temperature`).\n",
    "\n",
    "In other words: `top_p` controls the breadth of vocabulary in a generation and `temperature` controls the randomness within that vocabulary. A temperature of ~0 produces *almost* deterministic results.\n",
    "\n",
    "[Read more about temperature setting here](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy, gentle soul\n",
      "Llama's soft eyes gaze at me\n",
      "Peaceful Andean heart\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy, gentle soul\n",
      "Llama's soft eyes gaze at me\n",
      "Peaceful Andean heart\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Softly llama stands\n",
      "Fuzzy, gentle eyes gaze down\n",
      "Peaceful Andean king\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Furry, spiky pack\n",
      "Alpaca cousins roam freely\n",
      "Mountain's gentle soul\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tuned_completion(temperature: float, top_p: float):\n",
    "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
    "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
    "\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "# These two generations are highly likely to be the same\n",
    "\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "# These two generations are highly likely to be different"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Instructions\n",
    "\n",
    "Detailed, explicit instructions produce better results than open-ended prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Describe quantum physics in one short sentence of no more than 12 words\n",
      "==============\n",
      "Quantum physics studies the behavior of matter and energy at atomic level.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
    "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about giving explicit instructions as using rules and restrictions to how Llama 3 responds to your prompt.\n",
    "\n",
    "- Stylization\n",
    "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
    "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
    "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
    "- Formatting\n",
    "    - `Use bullet points.`\n",
    "    - `Return as a JSON object.`\n",
    "    - `Use less technical terms and help me apply it in my work in communications.`\n",
    "- Restrictions\n",
    "    - `Only use academic papers.`\n",
    "    - `Never give sources older than 2020.`\n",
    "    - `If you don't know the answer, say that you don't know.`\n",
    "\n",
    "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the latest advances in large language models to me.\n",
      "==============\n",
      "Large language models (LLMs) have made tremendous progress in recent years, and I'll summarize some of the key advancements:\n",
      "\n",
      "**1. Transformer Architecture**: The transformer architecture, introduced by Vaswani et al. in 2017, has become the de facto standard for LLMs. This architecture is based on self-attention mechanisms, which allow models to process sequences of tokens (e.g., words or characters) simultaneously and capture long-range dependencies.\n",
      "\n",
      "**2. Scaling Up**: As computational resources have improved, researchers have been able to scale up transformer-based models to unprecedented sizes. For example:\n",
      "\t* BERT (2018): 110 million parameters\n",
      "\t* RoBERTa (2019): 345 million parameters\n",
      "\t* T5 (2020): 1.3 billion parameters\n",
      "\t* Megatron-LM (2020): 8.4 billion parameters\n",
      "\t* Switch Transformer (2022): 1.6 trillion parameters\n",
      "\n",
      "**3. Pre-training and Fine-tuning**: The effectiveness of LLMs has been greatly enhanced by pre-training on large-scale datasets, followed by fine-tuning on specific tasks. This approach allows models to learn general language understanding and adapt to new tasks with minimal additional training.\n",
      "\n",
      "**4. Multitask Learning**: Researchers have explored multitask learning approaches, where a single model is trained on multiple tasks simultaneously (e.g., language translation, question-answering, text classification). This has led to better performance on individual tasks and improved robustness.\n",
      "\n",
      "**5. Few-shot Learning**: The ability of LLMs to learn from few examples, rather than requiring large datasets, has been demonstrated in various studies. This is particularly useful for low-resource languages or domains where data scarcity is an issue.\n",
      "\n",
      "**6. Improved Efficiency**: Researchers have explored methods to reduce the computational and memory requirements of LLMs, such as:\n",
      "\t* Quantization: reducing precision from 32-bit floating-point numbers to 8-bit integers\n",
      "\t* Knowledge distillation: transferring knowledge from large models to smaller ones\n",
      "\t* Efficient transformer architectures (e.g., sparse transformers)\n",
      "\n",
      "**7. Attention Mechanisms**: The development of attention mechanisms has enabled LLMs to focus on specific parts of the input sequence, improving performance on tasks that require selective processing.\n",
      "\n",
      "**8. Large-scale Datasets**: The availability of massive datasets (e.g., Common Crawl, Wikipedia) has facilitated the training of large models and pushed the boundaries of what is possible with LLMs.\n",
      "\n",
      "Some notable recent advances include:\n",
      "\n",
      "* **PaLM 3B** (2022): a 540 billion parameter model that achieved state-of-the-art results on various tasks.\n",
      "* **Switch Transformer** (2022): a highly efficient transformer architecture that demonstrated competitive performance to larger models while using significantly fewer parameters.\n",
      "* **Optimizing Language Models for Efficiency** (2022): research exploring ways to improve the efficiency of LLMs, such as through knowledge distillation and model pruning.\n",
      "\n",
      "Keep in mind that these advances are often built upon each other, and researchers continue to push the boundaries of what is possible with large language models.\n",
      "\n",
      "==============\n",
      "Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\n",
      "==============\n",
      "Here are some of the latest advances in large language models, along with their sources:\n",
      "\n",
      "1. **Scaling up to 10B parameters**: In a paper published in January 2022, researchers at Google introduced a new transformer-based model called \"T5\" that achieved state-of-the-art results on various natural language processing (NLP) tasks [1]. The T5 model has 10 billion parameters, making it one of the largest language models to date. This increase in scale was made possible by improvements in computing resources and optimization algorithms.\n",
      "2. **Efficient transformer architectures**: A study published in March 2022 proposed a new efficient transformer architecture called \"Sparse Transformer\" that reduces the number of parameters while maintaining performance [2]. The Sparse Transformer uses a sparsity-inducing mechanism to reduce the computational complexity, making it more suitable for large-scale language models.\n",
      "3. **Multitask learning**: Researchers at Meta AI introduced a multitask learning approach in June 2021 that allows large language models to learn multiple tasks simultaneously [3]. This approach was shown to improve performance on individual tasks and enable transfer learning across tasks.\n",
      "4. **Self-supervised pre-training**: A study published in October 2020 proposed a self-supervised pre-training method for large language models, which uses masked language modeling as the primary objective [4]. This approach has been widely adopted in recent NLP research and has led to significant improvements in performance on various tasks.\n",
      "5. **Advances in attention mechanisms**: Researchers at Google introduced a new attention mechanism called \"Longformer\" in October 2020 that can handle long-range dependencies without increasing the computational complexity [5]. The Longformer uses a combination of local and global attention mechanisms to improve performance on tasks such as text classification and question answering.\n",
      "6. **Improved training methods**: A study published in January 2022 proposed a new training method called \"Dense Knowledge Distillation\" that improves the quality of large language models by leveraging knowledge from smaller student models [6]. This approach has been shown to reduce the gap between small and large language models.\n",
      "\n",
      "These are just a few examples of recent advances in large language models. The field is rapidly evolving, and new breakthroughs are being reported regularly.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] Liu et al. (2022). T5: A Text-to-Text Transfer Transformer. arXiv preprint arXiv:2201.00261.\n",
      "\n",
      "[2] Wang et al. (2022). Sparse Transformer: Efficient Transformers with Learned Sparsity. arXiv preprint arXiv:2203.01073.\n",
      "\n",
      "[3] Lewis et al. (2021). Emergent Abilities of Large Language Models. arXiv preprint arXiv:2110.07850.\n",
      "\n",
      "[4] Devlin et al. (2020). BERT, RoBERTa and the Limitations of Counterfactual Reasoning. arXiv preprint arXiv:2012.10812.\n",
      "\n",
      "[5] Beltagy et al. (2020). Longformer: The Long-Document Transformer. arXiv preprint arXiv:2004.05150.\n",
      "\n",
      "[6] Tang et al. (2022). Dense Knowledge Distillation for Large Language Models. arXiv preprint arXiv:2201.00515.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the latest advances in large language models to me.\")\n",
    "# More likely to cite sources from 2017\n",
    "\n",
    "complete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n",
    "# Gives more specific advances and only cites sources from 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Prompting using Zero- and Few-Shot Learning\n",
    "\n",
    "A shot is an example or demonstration of what type of prompt and response you expect from a large language model. This term originates from training computer vision models on photographs, where one shot was one example or instance that the model used to classify an image ([Fei-Fei et al. (2006)](http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf)).\n",
    "\n",
    "#### Zero-Shot Prompting\n",
    "\n",
    "Large language models like Llama 3 are unique because they are capable of following instructions and producing responses without having previously seen an example of a task. Prompting without examples is called \"zero-shot prompting\".\n",
    "\n",
    "Let's try using Llama 3 as a sentiment detector. You may notice that output format varies - we can improve this with better prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Text: This was the best movie I've ever seen! \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "The sentiment of the text is extremely positive, indicating a strong enthusiasm and admiration for the movie. The use of the superlative \"best\" emphasizes the speaker's high opinion of the film.\n",
      "\n",
      "==============\n",
      "Text: The director was trying too hard. \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "Negative.\n",
      "\n",
      "The phrase \"trying too hard\" implies that the director's efforts are coming across as insincere, over-the-top, or even annoying, which suggests a negative opinion about their work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
    "# Returns positive sentiment\n",
    "\n",
    "complete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
    "# Returns negative sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Few-Shot Prompting\n",
    "\n",
    "Adding specific examples of your desired output generally results in more accurate, consistent output. This technique is called \"few-shot prompting\".\n",
    "\n",
    "In this example, the generated response follows our desired format that offers a more nuanced sentiment classifer that gives a positive, neutral, and negative response confidence percentage.\n",
    "\n",
    "See also: [Zhao et al. (2021)](https://arxiv.org/abs/2102.09690), [Liu et al. (2021)](https://arxiv.org/abs/2101.06804), [Su et al. (2022)](https://arxiv.org/abs/2209.01975), [Rubin et al. (2022)](https://arxiv.org/abs/2112.08633).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: I thought it was okay\n",
      "20% positive 70% neutral 10% negative\n",
      "INPUT: I loved it!\n",
      "95% positive 5% neutral 0% negative\n",
      "INPUT: Terrible service 0/10\n",
      "0% positive 0% neutral 100% negative\n"
     ]
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    response = chat_completion(messages=[\n",
    "        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n",
    "        user(\"I liked it\"),\n",
    "        assistant(\"70% positive 30% neutral 0% negative\"),\n",
    "        user(\"It could be better\"),\n",
    "        assistant(\"0% positive 50% neutral 50% negative\"),\n",
    "        user(\"It's fine\"),\n",
    "        assistant(\"25% positive 50% neutral 25% negative\"),\n",
    "        user(text),\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "def print_sentiment(text):\n",
    "    print(f'INPUT: {text}')\n",
    "    print(sentiment(text))\n",
    "\n",
    "print_sentiment(\"I thought it was okay\")\n",
    "# More likely to return a balanced mix of positive, neutral, and negative\n",
    "print_sentiment(\"I loved it!\")\n",
    "# More likely to return 100% positive\n",
    "print_sentiment(\"Terrible service 0/10\")\n",
    "# More likely to return 100% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Llama will often give more consistent responses when given a role ([Kong et al. (2023)](https://browse.arxiv.org/pdf/2308.07702.pdf)). Roles give context to the LLM on what type of answers are desired.\n",
    "\n",
    "Let's use Llama 3 to create a more focused, technical response for a question around the pros and cons of using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "**Pros of Using PyTorch:**\n",
      "\n",
      "### 1. **Dynamic Computation Graph**\n",
      "PyTorch's computation graph is built dynamically at runtime, which allows for faster model prototyping and easier debugging.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "x = torch.tensor(2)\n",
      "y = x + 3  # Dynamic computation graph created automatically\n",
      "print(y)   # Output: tensor(5)\n",
      "```\n",
      "\n",
      "### 2. **Pythonic API**\n",
      "PyTorch's API is designed to be intuitive and Pythonic, making it easy for developers familiar with Python to learn and use.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "model = torch.nn.Linear(5, 3)  # Simple linear model created in a few lines of code\n",
      "```\n",
      "\n",
      "### 3. **Automatic Differentiation**\n",
      "PyTorch's automatic differentiation (autograd) system enables seamless computation of gradients for training neural networks.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "x = torch.tensor(2)\n",
      "y = x + 3\n",
      "\n",
      "# Automatic gradient computation enabled by autograd\n",
      "y.backward()\n",
      "print(x.grad)  # Output: tensor(1.0)\n",
      "```\n",
      "\n",
      "### 4. **Torchvision and Torchtext**\n",
      "PyTorch provides pre-built modules for computer vision (Torchvision) and natural language processing (Torchtext), making it easier to work with images and text data.\n",
      "\n",
      "```python\n",
      "import torchvision\n",
      "\n",
      "# Load a sample image dataset using torchvision\n",
      "dataset = torchvision.datasets.ImageFolder('path/to/dataset')\n",
      "```\n",
      "\n",
      "### 5. **Large Community and Resources**\n",
      "PyTorch has an active community, extensive documentation, and numerous pre-trained models available for various tasks, making it easier to get started and find help when needed.\n",
      "\n",
      "**Cons of Using PyTorch:**\n",
      "\n",
      "### 1. **Steep Learning Curve for Advanced Users**\n",
      "While PyTorch is generally easy to learn, its flexibility and customizability can lead to a steeper learning curve for users who need to perform complex or customized operations.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Complex model definition may require additional setup\n",
      "class CustomModel(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.fc1 = torch.nn.Linear(5, 10)\n",
      "        self.fc2 = torch.nn.Linear(10, 3)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        return self.fc2(x)\n",
      "```\n",
      "\n",
      "### 2. **Limited Support for Distributed Training**\n",
      "While PyTorch provides some support for distributed training, it may not be as comprehensive or efficient as other frameworks (e.g., TensorFlow).\n",
      "\n",
      "```python\n",
      "import torch.distributed\n",
      "\n",
      "# Distributed training setup requires additional configuration\n",
      "torch.distributed.init_process_group('gloo', init_method='env://')\n",
      "```\n",
      "\n",
      "### 3. **Not Suitable for Large-Scale Production Deployments**\n",
      "PyTorch is generally not recommended for large-scale production deployments due to its focus on research and development rather than scalability and performance.\n",
      "\n",
      "```python\n",
      "# PyTorch models are typically not optimized for large-scale production use cases\n",
      "model = torch.nn.Linear(5, 3)\n",
      "```\n",
      "\n",
      "In conclusion, while PyTorch has some drawbacks, its dynamic computation graph, Pythonic API, automatic differentiation, and extensive community resources make it a popular choice among researchers and developers. However, users should be aware of the potential limitations and consider their specific needs before selecting PyTorch as their deep learning framework of choice.\n",
      "\n",
      "==============\n",
      "Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "As a machine learning expert, I've had the pleasure of working with various frameworks, including PyTorch. Here's a detailed breakdown of its pros and cons:\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Dynamic computation graph**: PyTorch uses a dynamic computation graph, which means that it builds the computation graph on-the-fly as you execute your code. This allows for more flexibility and easier debugging compared to static computation graphs used in frameworks like TensorFlow.\n",
      "2. **Automatic differentiation**: PyTorch's automatic differentiation (AD) system makes it simple to compute gradients with respect to any variable in your model, which is essential for training neural networks.\n",
      "3. **Pythonic API**: PyTorch has a clean and Pythonic API that's easy to use and understand, even for those without extensive experience with machine learning frameworks.\n",
      "4. **Large community**: PyTorch has an active and growing community, which means there are many resources available online, including tutorials, blogs, and forums.\n",
      "5. **Flexibility**: PyTorch allows you to easily mix and match different types of neural networks, such as convolutional, recurrent, or transformer-based models.\n",
      "6. **Lightweight**: Compared to other frameworks like TensorFlow or Keras, PyTorch is relatively lightweight and easy to install.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Limited support for distributed training**: While PyTorch has improved its support for distributed training in recent versions, it still lags behind other frameworks like TensorFlow or Horovod.\n",
      "2. **Not as mature as some other frameworks**: Compared to TensorFlow or Keras, which have been around longer and have more extensive feature sets, PyTorch is relatively new and may not offer the same level of maturity.\n",
      "3. **Limited support for certain hardware**: PyTorch has limited support for certain types of hardware, such as NVIDIA's older GPU architectures (e.g., Tesla or Fermi).\n",
      "4. **Steep learning curve for some concepts**: While PyTorch is generally easy to use, some advanced concepts like custom autograd functions or using the `torch.autograd.grad` function can be tricky to grasp.\n",
      "5. **Not ideal for large-scale production environments**: While PyTorch is great for prototyping and research, it may not be the best choice for large-scale production environments due to its limited support for distributed training and other scalability features.\n",
      "\n",
      "**When to choose PyTorch:**\n",
      "\n",
      "1. **Rapid prototyping**: If you need to quickly test a new idea or prototype an algorithm, PyTorch is an excellent choice.\n",
      "2. **Research-oriented projects**: For research-oriented projects that require flexibility and customization, PyTorch's dynamic computation graph and automatic differentiation make it an attractive option.\n",
      "3. **Small-scale production environments**: If your project requires only a few GPUs or doesn't have strict scalability requirements, PyTorch can be a good choice.\n",
      "\n",
      "**When to choose other frameworks:**\n",
      "\n",
      "1. **Large-scale production environments**: For large-scale production environments with multiple GPUs and strict scalability requirements, TensorFlow, Horovod, or Keras might be more suitable.\n",
      "2. **Distributed training on HPC clusters**: If you need to distribute your model across a high-performance computing (HPC) cluster, frameworks like TensorFlow or Horovod are better equipped to handle this.\n",
      "\n",
      "Ultimately, the choice of framework depends on your specific use case and project requirements. As a senior engineer working with complicated datasets, I recommend considering factors like scalability, distributed training support, and ease of use when deciding between PyTorch and other frameworks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the pros and cons of using PyTorch.\")\n",
    "# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n",
    "\n",
    "complete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought\n",
    "\n",
    "Simply adding a phrase encouraging step-by-step thinking \"significantly improves the ability of large language models to perform complex reasoning\" ([Wei et al. (2022)](https://arxiv.org/abs/2201.11903)). This technique is called \"CoT\" or \"Chain-of-Thought\" prompting.\n",
    "\n",
    "Llama 3.1 now reasons step-by-step naturally without the addition of the phrase. This section remains for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Who lived longer, Mozart or Elvis?\n",
      "==============\n",
      "A classic question!\n",
      "\n",
      "Wolfgang Amadeus Mozart (1756-1791) was a composer who passed away at the age of 35.\n",
      "\n",
      "Elvis Presley (1935-1977), on the other hand, died at the age of 42.\n",
      "\n",
      "So, Elvis outlived Mozart by about 7 years.\n",
      "\n",
      "==============\n",
      "Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\n",
      "==============\n",
      "Let's analyze the timeline of both lives to determine who lived longer.\n",
      "\n",
      "**Step 1: Birth and Death Dates**\n",
      "\n",
      "* Wolfgang Amadeus Mozart (1723-1791)\n",
      "\t+ Born on January 27, 1756\n",
      "\t+ Died on December 5, 1791 (at age 35)\n",
      "* Elvis Presley (1935-1977)\n",
      "\t+ Born on January 8, 1935\n",
      "\t+ Died on August 16, 1977 (at age 42)\n",
      "\n",
      "**Step 2: Comparison**\n",
      "\n",
      "Now that we have the birth and death dates for both individuals, let's compare them.\n",
      "\n",
      "* Mozart was born in 1756 and died in 1791, which means he lived from approximately 35 years.\n",
      "* Elvis was born in 1935 and died in 1977, which means he lived for approximately 42 years.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the calculations, Elvis Presley (1935-1977) lived longer than Wolfgang Amadeus Mozart (1756-1791), by about 7 years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who lived longer, Mozart or Elvis?\"\n",
    "\n",
    "complete_and_print(prompt)\n",
    "# Llama 2 would often give the incorrect answer of \"Mozart\"\n",
    "\n",
    "complete_and_print(f\"{prompt} Let's think through this carefully, step by step.\")\n",
    "# Gives the correct answer \"Elvis\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "LLMs are probablistic, so even with Chain-of-Thought, a single generation might produce incorrect results. Self-Consistency ([Wang et al. (2022)](https://arxiv.org/abs/2203.11171)) introduces enhanced accuracy by selecting the most frequent answer from multiple generations (at the cost of higher compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: ['50', '50', '50', None, '50']\n",
      "Final answer: 50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by backticks (example: `123`)\",\n",
    "    )\n",
    "    match = re.search(r'`(\\d+)`', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(f\"Answers: {answers}\\nFinal answer: {mode(answers)}\")\n",
    "\n",
    "# Sample runs of Llama-3-70B (all correct):\n",
    "# ['60', '50', '50', '50', '50'] -> 50\n",
    "# ['50', '50', '50', '60', '50'] -> 50\n",
    "# ['50', '50', '60', '50', '50'] -> 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "You'll probably want to use factual knowledge in your application. You can extract common facts from today's large models out-of-the-box (i.e. using just the model weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What is the capital of the California?\n",
      "==============\n",
      "The capital of California is Sacramento.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What is the capital of the California?\")\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more specific facts, or private information, cannot be reliably retrieved. The model will either declare it does not know or hallucinate an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What was the temperature in Menlo Park on December 12th, 2023?\n",
      "==============\n",
      "I'm a large language model, I don't have real-time access to current or historical weather data. However, I can suggest some ways for you to find out the temperature in Menlo Park on December 12th, 2023.\n",
      "\n",
      "You can try checking online weather websites such as:\n",
      "\n",
      "1. Weather.com\n",
      "2. AccuWeather.com\n",
      "3. National Weather Service (NWS) website\n",
      "\n",
      "These websites usually have historical weather data available, but it might take some time for the data from December 12th, 2023 to be updated and made public.\n",
      "\n",
      "Alternatively, you can also try checking with local news outlets or government websites in Menlo Park for more information.\n",
      "\n",
      "==============\n",
      "What time is my dinner reservation on Saturday and what should I wear?\n",
      "==============\n",
      "I'm a large language model, I don't have the ability to access personal information or specific details about your schedule. However, I can guide you through the process of finding out more.\n",
      "\n",
      "To find out the time of your dinner reservation on Saturday, you'll likely need to:\n",
      "\n",
      "1. Check your calendar: Look for any digital calendars (e.g., Google Calendar, Apple Calendar) or physical planners where you've scheduled the event.\n",
      "2. Contact the restaurant directly: Reach out to the restaurant's phone number or email to confirm the reservation details, including the time.\n",
      "3. Ask someone who knows: If you're with someone who helped make the reservation, ask them for the details.\n",
      "\n",
      "Regarding what to wear, that depends on the dress code and atmosphere of the restaurant. Here are a few suggestions:\n",
      "\n",
      "1. Check the restaurant's website or social media: Look for any information about their dress code or policies.\n",
      "2. Call the restaurant: Ask the staff if they have a specific dress code or recommendations for attire.\n",
      "3. Consider the occasion: If it's a special occasion (e.g., anniversary, birthday), you may want to dress up accordingly.\n",
      "\n",
      "If you're still unsure, feel free to provide more context or details about your reservation, and I'll do my best to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n",
    "# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n",
    "\n",
    "complete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n",
    "# \"I'm not able to access your personal information [..] I can provide some general guidance\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation, or RAG, describes the practice of including information in the prompt you've retrived from an external database ([Lewis et al. (2020)](https://arxiv.org/abs/2005.11401v4)). It's an effective way to incorporate facts into your LLM application and is more affordable than fine-tuning which may be costly and negatively impact the foundational model's capabilities.\n",
    "\n",
    "This could be as simple as a lookup table or as sophisticated as a [vector database](https://github.com/facebookresearch/faiss) containing all of your company's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was 51 degrees Fahrenheit on 2023-12-12'', respond to: 'What is the temperature in Menlo Park on 2023-12-12?'\n",
      "==============\n",
      "Based on the provided information, the temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\n",
      "\n",
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was unknown temperature on 2023-07-18'', respond to: 'What is the temperature in Menlo Park on 2023-07-18?'\n",
      "==============\n",
      "I don't have access to real-time or specific weather data, but I can tell you that the information provided states that the temperature in Menlo Park was unknown on 2023-07-18. Therefore, I cannot provide a specific temperature for that date and location. If you need accurate and up-to-date weather information, I recommend checking a reliable weather forecasting service or website.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-Aided Language Models\n",
    "\n",
    "LLMs, by nature, aren't great at performing calculations. Let's try:\n",
    "\n",
    "$$\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "$$\n",
    "\n",
    "(The correct answer is 91383.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "Calculate the answer to the following math problem:\n",
      "\n",
      "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "\n",
      "==============\n",
      "To calculate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expressions inside the parentheses:\n",
      "\t* `93 * 4 = 372`\n",
      "\t* `4^4 = 256`\n",
      "2. Replace these values in the original expression:\n",
      "\t* `((-5 + 372 - 0) * (256 + (-7) + 0 * 5))`\n",
      "3. Simplify inside the parentheses:\n",
      "\t* `-5 + 372 = 367`\n",
      "\t* `(256 - 7) = 249`\n",
      "\t* `0 * 5 = 0` (since any number multiplied by zero is zero)\n",
      "4. Now the expression becomes:\n",
      "\t* `(367 * (249 + 0))`\n",
      "5. Simplify inside the parentheses:\n",
      "\t* `249 + 0 = 249`\n",
      "6. Multiply 367 and 249:\n",
      "\t* `367 * 249 = 91413`\n",
      "\n",
      "So, the final answer is: **91413**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "# Gives incorrect answers like 92448, 92648, 95463"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gao et al. (2022)](https://arxiv.org/abs/2211.10435) introduced the concept of \"Program-aided Language Models\" (PAL). While LLMs are bad at arithmetic, they're great for code generation. PAL leverages this fact by instructing the LLM to write code to solve calculation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "    \n",
      "==============\n",
      "Here is the Python code that performs the calculation:\n",
      "```python\n",
      "result = (((-5) + (93 * 4)) - 0) * ((4**4) + (-7) + (0 * 5))\n",
      "print(result)\n",
      "```\n",
      "Explanation:\n",
      "\n",
      "* The expression `(-5 + 93 * 4 - 0)` is evaluated first, following the order of operations. This equals `-5 + 372 = 367`.\n",
      "* Next, `(4^4 + -7 + 0 * 5)` is evaluated. Since `^` denotes exponentiation in Python, this equals `256 + (-7) = 249`.\n",
      "* Finally, the result of the first expression (`367`) is multiplied by the result of the second expression (`249`), yielding a final answer.\n",
      "\n",
      "Running this code will output:\n",
      "```\n",
      "91313\n",
      "```\n",
      "Let me know if you have any questions or need further clarification!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91383\n"
     ]
    }
   ],
   "source": [
    "# The following code was generated by Llama 3 70B:\n",
    "\n",
    "result = ((-5 + 93 * 4 - 0) * (4**4 - 7 + 0 * 5))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle with Llama 2 is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\"), even if explicit instructions are given to Llama 2 to be concise and no preamble. Llama 3.x can better follow instructions.\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\n",
      "==============\n",
      "Here is a single JSON object containing the zip code for Menlo Park:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"zip_code\": \"94025\"\n",
      "}\n",
      "```\n",
      "\n",
      "==============\n",
      "\n",
      "    You are a robot that only outputs JSON.\n",
      "    You reply in JSON format with the field 'zip_code'.\n",
      "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
      "    Now here is my question: What is the zip code of Menlo Park?\n",
      "    \n",
      "==============\n",
      "{\"zip_code\": \"94025\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    ")\n",
    "# \"{'zip_code': 94025}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
    "- [LearnPrompting.org](https://learnprompting.org/)\n",
    "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
   ]
  }
 ],
 "metadata": {
  "captumWidgetMessage": [],
  "dataExplorerConfig": [],
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
  "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
  "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
  "operator_data": [],
  "outputWidgetContext": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
